import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Tuple, Dict, Any
from docx import Document
import time
from time import sleep
import datetime
import sys
from tqdm import tqdm
from openai import OpenAI, OpenAIError, APIError, RateLimitError, APIConnectionError, Timeout

DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")
DEEPSEEK_BASE_URL = os.getenv("DEEPSEEK_BASE_URL", "")
DEEPSEEK_MODEL = "deepseek-chat" 
# My macmini has 8 core -> can run 10 threads comfortably
MAX_WORKERS = int(os.getenv("MAX_WORKERS", "10"))
WORD_LIMIT = int(os.getenv("BATCH_WORD_LIMIT", "400"))

def chunk_texts_by_words(texts: List[str], word_limit: int = WORD_LIMIT) -> List[List[str]]:
    """Gom nh√≥m c√°c ƒëo·∫°n vƒÉn th√†nh c√°c batch sao cho t·ªïng s·ªë t·ª´ c·ªßa m·ªói batch kh√¥ng v∆∞·ª£t qu√° word_limit."""
    
    # Kh·ªüi t·∫°o danh s√°ch ch·ª©a c√°c batch, batch hi·ªán t·∫°i v√† s·ªë t·ª´ trong batch hi·ªán t·∫°i
    batches, current_batch, current_count = [], [], 0
    
    # L·∫∑p qua t·ª´ng ƒëo·∫°n vƒÉn trong danh s√°ch texts
    for t in texts:
        # T√≠nh s·ªë t·ª´ c·ªßa ƒëo·∫°n vƒÉn hi·ªán t·∫°i b·∫±ng c√°ch t√°ch theo kho·∫£ng tr·∫Øng
        wc = len(t.split())
        
        # N·∫øu ƒëo·∫°n vƒÉn c√≥ s·ªë t·ª´ l·ªõn h∆°n gi·ªõi h·∫°n v√† batch hi·ªán t·∫°i ƒëang r·ªóng,
        # nghƒ©a l√† kh√¥ng th·ªÉ g·ªôp v·ªõi ƒëo·∫°n n√†o kh√°c, th√¨ th√™m ƒëo·∫°n n√†y v√†o m·ªôt batch ri√™ng
        if wc > word_limit and not current_batch:
            batches.append([t])
            continue
        
        # N·∫øu s·ªë t·ª´ sau khi th√™m ƒëo·∫°n hi·ªán t·∫°i v∆∞·ª£t qu√° gi·ªõi h·∫°n v√† batch hi·ªán t·∫°i kh√¥ng r·ªóng,
        # th√¨ l∆∞u batch hi·ªán t·∫°i v√† b·∫Øt ƒë·∫ßu m·ªôt batch m·ªõi ch·ª©a ƒëo·∫°n n√†y
        if current_count + wc > word_limit and current_batch:
            batches.append(current_batch)
            current_batch, current_count = [t], wc
        else:
            # N·∫øu kh√¥ng v∆∞·ª£t qu√° gi·ªõi h·∫°n, th√™m ƒëo·∫°n v√†o batch hi·ªán t·∫°i v√† c·∫≠p nh·∫≠t s·ªë t·ª´
            current_batch.append(t)
            current_count += wc
    
    # Sau v√≤ng l·∫∑p, n·∫øu c√≤n batch kh√¥ng ƒë∆∞·ª£c th√™m v√†o danh s√°ch batches, th√™m n√≥ v√†o
    if current_batch:
        batches.append(current_batch)
    
    return batches


def translate_batch_openai(client: OpenAI, texts: List[str]) -> List[str]:
    """D·ªãch 1 batch text qua OpenAI API (DeepSeek-compatible n·∫øu c√≥ base_url)."""
    outputs = []
    
    
    for t in texts:
        translated = translate_text(client, t)
        outputs.append(translated)
    return outputs

def translate_text(client: OpenAI, text: str, temperature: float = 0.7) -> str:
    """
    D·ªãch `text` sang `target_lang` b·∫±ng DeepSeek API.
    Ch·ªâ tr·∫£ v·ªÅ ph·∫ßn d·ªãch, kh√¥ng c√≥ l·ªùi gi·∫£i th√≠ch n√†o kh√°c.
    """
    if not text.strip():
        return text

    # Chu·∫©n b·ªã prompt (system + user)
    messages = [
    {
        "role": "system",
        "content": (
            "You are a professional translator specializing in poker literature and strategy. "
            "Your task is to translate the given English text into natural, fluent, and accurate Vietnamese, "
            "while preserving the original meaning, tone, and technical poker terminology.\n\n"
            "Translation rules:\n"
            "1. Translate exactly ‚Äî do not add, omit, interpret, or explain anything beyond what is written.\n"
            "2. Keep all poker-related terms (such as range, GTO, EV, range, equity, bluff, combo, exploitative play, solver, polarize, depolarize, merge, capped range, uncapped range, pot odds, value bet, thin value, overbet, underbet, c-bet, 3-bet, 4-bet, flat call, limp, squeeze, board texture, equity realization, blocker, nut advantage, range advantage, hand, flop, turn, river, value bet, node blockers, pot multiway, multiway, high card, kicker, suited connectors, pocket pairs, one-gappers, two-gappers, backdoor flush draw, gutshot straight draw, open-ended straight draw, overcards, undercards, top pair, second pair, bottom pair, middle pair, overpair, set, trips, full house, flush, straight, wheel, nut flush, nut straight, cooler, bad beat)"
            "either in English or translated to their most common Vietnamese equivalent if one exists naturally.\n"
            "3. Maintain the original paragraph structure and sentence flow.\n"
            "4. Ensure the translation sounds professional, readable, and natural in Vietnamese, not like a machine translation.\n"
            "5. Do not include commentary, analysis, or summaries. Output only the translated text.\n\n"
            "Context conditions to improve translation quality:\n"
            "- The source text comes from a professional poker strategy book for intermediate to advanced players.\n"
            "- Prioritize accuracy in strategic and mathematical terms (e.g., pot odds, equity realization, bet sizing).\n"
            "- Use neutral and formal tone suitable for educational or instructional writing.\n"
            "- If a phrase has no direct equivalent in Vietnamese, keep it in English and translate the surrounding context naturally.\n"
            "- Pay attention to nuances that may affect meaning (e.g., exploitative vs GTO, range advantage, polarized hands)."
        )
    },
    {
        "role": "user",
        "content": (
            "Input:\n" + text
        )
    }
]
    for attempt in range(1, 4):
            try:
                response = client.chat.completions.create(
                    model=DEEPSEEK_MODEL,
                    messages=messages,
                    temperature=temperature,
                    timeout=30,
                    stream=False# gi√¢y
                )
                translated = response.choices[0].message.content.strip()
                return translated or text  # n·∫øu API tr·∫£ r·ªóng th√¨ fallback v·ªÅ text g·ªëc

            except (APIError, APIConnectionError, Timeout, RateLimitError) as e:
                print(f"[‚ö†Ô∏è API Error: - Attempt {attempt}/{3}] {e}")
                if attempt < 3:
                    time.sleep(2 * attempt)
                    continue
                else:
                    print("[‚ùå API Error: DeepSeek API failed ‚Äî returning original text]")
                    return text  # fallback

            except OpenAIError as e:
                print(f"[‚ùå API Error: SDK Error: {e}]")
                return text  # fallback
            except Exception as e:
                print(f"[üí• API Error: Unexpected Error: {e}]")
                return text  # fallback
    # N·∫øu h·∫øt retries m√† v·∫´n kh√¥ng th√†nh c√¥ng
    return text

# ----------------------
# DOCX Processing
# ----------------------
def collect_translatable_items(doc: Document) -> List[Tuple[str, object]]:
    """Thu th·∫≠p t·∫•t c·∫£ ƒëo·∫°n vƒÉn c·∫ßn d·ªãch (paragraph + table cell)."""
    items = []
    for p in doc.paragraphs:
        txt = p.text.strip()
        if txt:
            items.append((txt, ("paragraph", p)))
    for table in doc.tables:
        for r in table.rows:
            for c in r.cells:
                cell_text = "\n".join([pp.text for pp in c.paragraphs]).strip()
                if cell_text:
                    items.append((cell_text, ("table-cell", c)))
    return items

def get_run_style(run):
    """Tr√≠ch xu·∫•t c√°c thu·ªôc t√≠nh style c·ªßa run v√† tr·∫£ v·ªÅ dict."""
    return {
        "bold": run.bold,
        "italic": run.italic,
        "underline": run.underline,
        "style": run.style,
        "font_name": run.font.name,
        "font_size": run.font.size,
        "color": run.font.color.rgb if run.font.color else None,
    }

def apply_translations(doc: Document, translations_map: dict):
    """Thay th·∫ø vƒÉn b·∫£n b·∫±ng b·∫£n d·ªãch trong docx (·∫£nh & b·∫£ng gi·ªØ nguy√™n)."""
    for p in doc.paragraphs:
        if p.text and p.text in translations_map:
            new_text = translations_map[p.text]
            runs_data = []
            for run in p.runs:
                runs_data.append(get_run_style(run=run))
            p.clear()
            new_run = p.add_run(new_text)
            for item in runs_data:
                new_run.bold = item["bold"]
                new_run.italic = item["italic"]
                new_run.underline = item["underline"]
                new_run.style = item["style"]
                new_run.font.name = item["font_name"]

    for table in doc.tables:
        for r in table.rows:
            for c in r.cells:
                original = "\n".join([pp.text for pp in c.paragraphs])
                if original and original in translations_map:
                    translated = translations_map[original]
                    if c.paragraphs:
                        first = c.paragraphs[0]
                        first.clear()
                        first.add_run(translated)


# ----------------------
# Main translation
# ----------------------
def translate_docx_file(input_path: str, output_path: str = None, show_progress: bool = True):
    """
    D·ªãch file docx v·ªõi hi·ªÉn th·ªã ti·∫øn ƒë·ªô chi ti·∫øt.
    
    Args:
        input_path: ƒê∆∞·ªùng d·∫´n file input
        output_path: ƒê∆∞·ªùng d·∫´n file output (m·∫∑c ƒë·ªãnh l√† {t√™n_file}_vi.docx)
        show_progress: Hi·ªÉn th·ªã thanh ti·∫øn ƒë·ªô (m·∫∑c ƒë·ªãnh l√† True)
    """
    if not DEEPSEEK_API_KEY:
        raise RuntimeError("‚ùå OPENAI_API_KEY ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p.")

    # Hi·ªÉn th·ªã th√¥ng tin b·∫Øt ƒë·∫ßu
    start_time = time.time()
    print(f"üöÄ B·∫Øt ƒë·∫ßu d·ªãch file: {os.path.basename(input_path)}")
    print(f"‚öôÔ∏è C·∫•u h√¨nh: MAX_WORKERS={MAX_WORKERS}, WORD_LIMIT={WORD_LIMIT}")
    
    client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL or None)

    if output_path is None:
        base, ext = os.path.splitext(input_path)
        output_path = f"{base}_vi{ext}"

    # ƒê·ªçc t√†i li·ªáu
    print(f"üìÑ ƒêang ƒë·ªçc t√†i li·ªáu: {input_path}")
    doc = Document(input_path)
    
    # Thu th·∫≠p c√°c ph·∫ßn t·ª≠ c·∫ßn d·ªãch
    print("üîç ƒêang ph√¢n t√≠ch t√†i li·ªáu...")
    items = collect_translatable_items(doc)
    if not items:
        print("‚ö†Ô∏è Kh√¥ng c√≥ ƒëo·∫°n vƒÉn b·∫£n n√†o c·∫ßn d·ªãch.")
        doc.save(output_path)
        return output_path

    # Lo·∫°i b·ªè tr√πng l·∫∑p
    unique_texts = []
    for txt, _ in items:
        if txt not in unique_texts:
            unique_texts.append(txt)

    total_texts = len(unique_texts)
    total_words = sum(len(text.split()) for text in unique_texts)
    
    print(f"üìä Th·ªëng k√™:")
    print(f"  - T·ªïng ƒëo·∫°n c·∫ßn d·ªãch: {total_texts}")
    print(f"  - T·ªïng s·ªë t·ª´: {total_words}")

    # Chia th√†nh c√°c batch
    batches = chunk_texts_by_words(unique_texts, word_limit=WORD_LIMIT)
    total_batches = len(batches)
    print(f"üì¶ ƒê√£ chia th√†nh {total_batches} batch ƒë·ªÉ x·ª≠ l√Ω")
    
    # Kh·ªüi t·∫°o bi·∫øn theo d√µi ti·∫øn ƒë·ªô
    translations_map = {}
    completed_batches = 0
    completed_texts = 0
    
    # Hi·ªÉn th·ªã thanh ti·∫øn ƒë·ªô t·ªïng th·ªÉ
    if show_progress:
        progress_bar = tqdm(total=total_texts, desc="Ti·∫øn ƒë·ªô d·ªãch", unit="ƒëo·∫°n")

    # S·ª≠ d·ª•ng ThreadPoolExecutor ƒë·ªÉ x·ª≠ l√Ω ƒëa lu·ªìng
    print(f"‚ö° B·∫Øt ƒë·∫ßu d·ªãch v·ªõi {MAX_WORKERS} lu·ªìng song song...")
    batch_start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as exe:
        # T·∫°o dictionary √°nh x·∫° Future objects v·ªõi c√°c batch
        future_to_batch = {exe.submit(translate_batch_openai, client, b): b for b in batches}
        
        # L·∫∑p qua c√°c Future objects khi ho√†n th√†nh
        for fut in as_completed(future_to_batch):
            # L·∫•y batch t∆∞∆°ng ·ª©ng v·ªõi Future ƒë√£ ho√†n th√†nh
            batch = future_to_batch[fut]
            batch_size = len(batch)
            
            try:
                # L·∫•y k·∫øt qu·∫£ d·ªãch
                results = fut.result()
                
                # C·∫≠p nh·∫≠t ti·∫øn ƒë·ªô
                completed_batches += 1
                completed_texts += batch_size
                
                # Hi·ªÉn th·ªã th√¥ng tin ti·∫øn ƒë·ªô
                batch_time = time.time() - batch_start_time
                batch_words = sum(len(text.split()) for text in batch)
                
                # In th√¥ng tin ti·∫øn ƒë·ªô
                print(f"‚úÖ Batch {completed_batches}/{total_batches} ho√†n th√†nh: {batch_size} ƒëo·∫°n, {batch_words} t·ª´ trong {batch_time:.2f}s")
                print(f"   Ti·∫øn ƒë·ªô: {completed_texts}/{total_texts} ƒëo·∫°n ({completed_texts/total_texts*100:.1f}%)")
                
                # C·∫≠p nh·∫≠t thanh ti·∫øn ƒë·ªô
                if show_progress:
                    progress_bar.update(batch_size)
                
                # Reset th·ªùi gian cho batch ti·∫øp theo
                batch_start_time = time.time()
                
            except Exception as e:
                # X·ª≠ l√Ω l·ªói batch
                print(f"‚ö†Ô∏è L·ªói batch {completed_batches+1}/{total_batches}: {e}")
                print(f"   ‚Üí Fallback: d·ªãch t·ª´ng ƒëo·∫°n m·ªôt...")
                
                results = []
                # X·ª≠ l√Ω t·ª´ng ƒëo·∫°n vƒÉn trong batch
                for i, t in enumerate(batch):
                    try:
                        # Th·ª≠ d·ªãch t·ª´ng ƒëo·∫°n ri√™ng l·∫ª
                        res = translate_batch_openai(client, [t])[0]
                        print(f"   ‚úì ƒêo·∫°n {i+1}/{len(batch)} th√†nh c√¥ng")
                    except Exception as inner_e:
                        # N·∫øu v·∫´n l·ªói, gi·ªØ nguy√™n vƒÉn b·∫£n g·ªëc
                        res = t
                        print(f"   ‚úó ƒêo·∫°n {i+1}/{len(batch)} th·∫•t b·∫°i: {inner_e}")
                    results.append(res)
                    
                    # C·∫≠p nh·∫≠t thanh ti·∫øn ƒë·ªô
                    if show_progress:
                        progress_bar.update(1)
                
                # C·∫≠p nh·∫≠t ti·∫øn ƒë·ªô sau khi x·ª≠ l√Ω fallback
                completed_batches += 1
                completed_texts += batch_size
            
            # C·∫≠p nh·∫≠t translations_map v·ªõi c√°c c·∫∑p (vƒÉn b·∫£n g·ªëc: b·∫£n d·ªãch)
            for src, tr in zip(batch, results):
                translations_map[src] = tr
                
            # Hi·ªÉn th·ªã ∆∞·ªõc t√≠nh th·ªùi gian c√≤n l·∫°i
            elapsed_time = time.time() - start_time
            estimated_total_time = elapsed_time / (completed_texts / total_texts) if completed_texts > 0 else 0
            remaining_time = max(0, estimated_total_time - elapsed_time)
            
            # ƒê·ªãnh d·∫°ng th·ªùi gian c√≤n l·∫°i
            remaining_str = str(datetime.timedelta(seconds=int(remaining_time)))
            print(f"‚è±Ô∏è ∆Ø·ªõc t√≠nh th·ªùi gian c√≤n l·∫°i: {remaining_str}")
            
    # ƒê√≥ng thanh ti·∫øn ƒë·ªô
    if show_progress:
        progress_bar.close()

    # √Åp d·ª•ng b·∫£n d·ªãch v√†o t√†i li·ªáu
    print("üîÑ ƒêang √°p d·ª•ng b·∫£n d·ªãch v√†o t√†i li·ªáu...")
    apply_translations(doc, translations_map)
    
    # L∆∞u t√†i li·ªáu
    print(f"üíæ ƒêang l∆∞u t√†i li·ªáu ƒë√£ d·ªãch: {output_path}")
    doc.save(output_path)
    
    # Hi·ªÉn th·ªã th·ªëng k√™ cu·ªëi c√πng
    end_time = time.time()
    total_time = end_time - start_time
    words_per_second = total_words / total_time if total_time > 0 else 0
    
    print("\nüìä Th·ªëng k√™ cu·ªëi c√πng:")
    print(f"  - T·ªïng ƒëo·∫°n ƒë√£ d·ªãch: {total_texts}")
    print(f"  - T·ªïng s·ªë t·ª´: {total_words}")
    print(f"  - Th·ªùi gian th·ª±c hi·ªán: {total_time:.2f} gi√¢y")
    print(f"  - T·ªëc ƒë·ªô d·ªãch: {words_per_second:.2f} t·ª´/gi√¢y")
    print(f"‚úÖ File ƒë√£ d·ªãch l∆∞u t·∫°i: {output_path}")
    
    return output_path


# ----------------------
# CLI
# ----------------------
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="D·ªãch file .docx qua OpenAI (ho·∫∑c DeepSeek endpoint t∆∞∆°ng th√≠ch).")
    parser.add_argument("input", help="ƒê∆∞·ªùng d·∫´n file .docx c·∫ßn d·ªãch")
    parser.add_argument("-o", "--output", help="ƒê∆∞·ªùng d·∫´n file .docx sau khi d·ªãch (tu·ª≥ ch·ªçn)")
    parser.add_argument("--no-progress", action="store_true", help="Kh√¥ng hi·ªÉn th·ªã thanh ti·∫øn ƒë·ªô")
    parser.add_argument("--workers", type=int, help=f"S·ªë lu·ªìng x·ª≠ l√Ω song song (m·∫∑c ƒë·ªãnh: {MAX_WORKERS})")
    parser.add_argument("--word-limit", type=int, help=f"Gi·ªõi h·∫°n s·ªë t·ª´ m·ªói batch (m·∫∑c ƒë·ªãnh: {WORD_LIMIT})")
    
    args = parser.parse_args()
    
    # C·∫≠p nh·∫≠t c·∫•u h√¨nh n·∫øu c√≥
    if args.workers:
        MAX_WORKERS = args.workers
    if args.word_limit:
        WORD_LIMIT = args.word_limit
    
    # Ch·∫°y d·ªãch v·ªõi c·∫•u h√¨nh ƒë√£ thi·∫øt l·∫≠p
    translate_docx_file(args.input, args.output, show_progress=not args.no_progress)